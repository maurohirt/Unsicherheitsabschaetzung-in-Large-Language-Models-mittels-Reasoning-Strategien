Project root: /home2/mauro.hirt/CoT-UQ
Running with parameters:
MODEL_ENGINE: llama3-1_8B
DATASET: 2WikimhQA
OUTPUT_PATH: /home2/mauro.hirt/CoT-UQ/output/llama3-1_8B/2WikimhQA
TEMPERATURE: 1.0
TRY_TIMES: 20
ARRAY_ID: 1
DATASET_SELECTED: 2WikimhQA
Ensuring model meta-llama/Llama-3.1-8B is cached...
===== GPU INFO BEFORE MODEL LOADING =====
memory.total [MiB], memory.used [MiB]
20470 MiB, 1 MiB
===== STARTING INFERENCE =====
---------------experiment args---------------
max_length_cot:256
try_times:20
temperature:1.0
dataset:2WikimhQA
datapath:None
hf_token:***REMOVED***
api_key:
model_engine:llama3-1_8B
uq_engine:probas-mean
model_path:llama3-1_8B
output_path:/home2/mauro.hirt/CoT-UQ/output/llama3-1_8B/2WikimhQA
test_start:0
test_end:full

---------------------------------------------
The Number of Different Questions:  1548
FATAL: inference_refining.py execution failed with exit code 1
