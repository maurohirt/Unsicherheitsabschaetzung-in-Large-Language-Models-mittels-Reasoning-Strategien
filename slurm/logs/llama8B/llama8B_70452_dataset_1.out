Project root: /home2/mauro.hirt/CoT-UQ
Running with parameters:
MODEL_ENGINE: llama3-1_8B
DATASET: 2WikimhQA
OUTPUT_PATH: /home2/mauro.hirt/CoT-UQ/output/llama3-1_8B/2WikimhQA
TEMPERATURE: 1.0
TRY_TIMES: 20
ARRAY_ID: 1
JOB_ID: 70452
===== MODEL SETUP =====
Using model path: /root/.cache/huggingface/models/llama3-8B
Created symlink: llama3-1_8B -> /root/.cache/huggingface/models/llama3-8B
===== GPU INFO BEFORE MODEL LOADING =====
memory.total [MiB], memory.used [MiB]
24576 MiB, 6 MiB
===== STARTING INFERENCE FOR DATASET: 2WikimhQA =====
---------------experiment args---------------
max_length_cot:128
try_times:20
temperature:1.0
dataset:2WikimhQA
datapath:None
api_key:
model_engine:llama3-1_8B
uq_engine:probas-mean
model_path:llama3-1_8B
output_path:/home2/mauro.hirt/CoT-UQ/output/llama3-1_8B/2WikimhQA
test_start:0
test_end:full

---------------------------------------------
The Number of Different Questions:  1548
Using symlink at /root/.cache/huggingface/symlinks/llama3-1_8B
Loading model from: /root/.cache/huggingface/symlinks/llama3-1_8B
===== GPU INFO AFTER INFERENCE =====
memory.total [MiB], memory.used [MiB]
24576 MiB, 6 MiB
===== INFERENCE COMPLETED SUCCESSFULLY FOR DATASET: 2WikimhQA =====
âœ… Inference job completed successfully for dataset: 2WikimhQA
Log files:
- Standard output: slurm/logs/llama8B/llama8B_70452_2WikimhQA.out
- Standard error:  slurm/logs/llama8B/llama8B_70452_2WikimhQA.err
