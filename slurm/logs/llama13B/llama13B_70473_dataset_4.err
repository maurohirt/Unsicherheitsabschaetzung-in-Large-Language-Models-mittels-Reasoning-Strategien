Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.71s/it]
  0%|          | 0/1318 [00:00<?, ?it/s]2025-05-06 15:53:17,363 : ##### This is the --1th-- Question #####
  0%|          | 0/1318 [00:17<?, ?it/s]
Traceback (most recent call last):
  File "/home2/mauro.hirt/CoT-UQ/inference_refining.py", line 226, in <module>
    llama_inference_refining()
  File "/home2/mauro.hirt/CoT-UQ/inference_refining.py", line 50, in llama_inference_refining
    outputs = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 3476, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
