Project root: /home2/mauro.hirt/CoT-UQ
Running with parameters:
MODEL_ENGINE: llama2-13b
DATASET: gsm8k
OUTPUT_PATH: /home2/mauro.hirt/CoT-UQ/output/llama2-13b/gsm8k
TEMPERATURE: 1.2
TRY_TIMES: 20
ARRAY_ID: 4
JOB_ID: 70473
===== MODEL SETUP =====
Using model path: /root/.cache/huggingface/models/llama2-13b
Created symlink: llama2-13b -> /root/.cache/huggingface/models/llama2-13b
===== GPU INFO BEFORE MODEL LOADING =====
memory.total [MiB], memory.used [MiB], name
20470 MiB, 2 MiB, NVIDIA RTX A4500
20470 MiB, 2 MiB, NVIDIA RTX A4500
===== STARTING INFERENCE FOR DATASET: gsm8k =====
---------------experiment args---------------
max_length_cot:128
try_times:20
temperature:1.2
dataset:gsm8k
datapath:None
api_key:
model_engine:llama2-13b
uq_engine:probas-mean
model_path:llama2-13b
output_path:/home2/mauro.hirt/CoT-UQ/output/llama2-13b/gsm8k
test_start:0
test_end:full

---------------------------------------------
The Number of Different Questions:  1318
Using symlink at /root/.cache/huggingface/symlinks/llama2-13b
Loading model from: /root/.cache/huggingface/symlinks/llama2-13b
FATAL: inference_refining.py execution failed with exit code 1
‚ùå Inference job failed with exit status 1 for dataset: gsm8k
Log files:
- Standard output: slurm/logs/llama13B/llama13B_70473_gsm8k.out
- Standard error:  slurm/logs/llama13B/llama13B_70473_gsm8k.err
