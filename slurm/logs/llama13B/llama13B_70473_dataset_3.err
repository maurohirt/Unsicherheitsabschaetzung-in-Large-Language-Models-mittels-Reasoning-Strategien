Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.61s/it]
  0%|          | 0/1000 [00:00<?, ?it/s]2025-05-06 15:52:13,119 : ##### This is the --1th-- Question #####
  0%|          | 0/1000 [00:49<?, ?it/s]
Traceback (most recent call last):
  File "/home2/mauro.hirt/CoT-UQ/inference_refining.py", line 226, in <module>
    llama_inference_refining()
  File "/home2/mauro.hirt/CoT-UQ/inference_refining.py", line 50, in llama_inference_refining
    outputs = model.generate(
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 3476, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
